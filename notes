
      nohup sh ./full_bench.put dc1onlyput  > ./report/full_bench.out.dc1onlyput  2>&1 &
        nohup sh ./full_bench.get dc1onlyget  > ./report/full_bench.out.dc1onlyget  2>&1 &

        nohup sh ./full_bench.put-extrem dc1onlyputextreme  > ./report/full_bench.out.dc1onlyput-extrem  2>&1 &
        nohup sh ./full_bench.get-extrem dc1onlygetextreme  > ./report/full_bench.out.dc1onlyget-extrem  2>&1 &

        nohup sh ./full_bench.put-background  dc1onlyputbackgroud  > ./report/full_bench.out.dc1onlyput-background  2>&1 &


        to cp results back to master:
           client:
                cd results
                tar cvf /tmp/r.tar .
                "cp" /tmp/r.tar to master

           master:
                cd result
                mkdir cloudian
                cd cloudian
                tar xvf /tmp/r.tar

  Util:
        remove all -r :
                s3cmd ls  | awk '{print "./s3cmd-remove " $3}' > com
                sh ./com
        du  all:
                s3cmd ls  | awk '{print "s3cmd du  " $3}' > com
                sh ./com

        remove all empty buckets  :
                s3cmd ls  | awk '{print "s3cmd rb  " $3}' > com
                sh ./com


============================================================================
====PartI Cluster Operations ===============================================
============================================================================

1. supervisor 
	http: IP:3080 
		login: scality/scality 
	
	10.203.0.11:3080 

	
2. config
	all 40x2 (bounded) 
	sut01: supper & client 
	sut02 - 07: 6 data nodes 
	sut03: RS3 
	sut02/04/05/06/07: S3  

	each data node: 
		3 demon  (5 HDD)  <== z1 
			+ 
		3 demon  (5 HDD)  <== z2 

	our configuration : EC8+4 
	 
3. other 
	pdsh -g all|s3|nodes 
 		. config: vi /etc/genders 

	docker: 
		docker ps 
 			. pdsh -g s3 docker ps  | grep cosbench
 		docker restart scality-cosbench
			. super: restore controller 
			. all s3 node : restore drivers 
 			. all 5 drivers :  pdsh -g s3 docker restart scality-cosbench


============================================================================
====PartII cosbench           ===============================================
============================================================================
1.  configuration and run 

  contrroller and driver config 
	cd /scality/ssd1/scality-cosbench/conf/cosbench
	cat controller.conf
		. divers info 

   work load: 
 	cd /scality/ssd1/scality-cosbench/archive/tmp/
	the last file:  <=== more study 

     ! it use port 18088, 18188,18288
	
     ! it caused "out of socket" issue, so stop all after running 
	 ss -t -a   > /tmp/a 
	 check close-w sockets 


  client load config : 
	512K: size obj, 
	6000 objs  
	driver: 5 
	read : 50 workers 
	write: 50 workers 
	r|w  : 100 workers 

3. opeations: 
    on super: 
	cd /home/cosbench/cos/scality/
	vi submit_sequential_cosbench_workload-s3.sh 
 	docker restart scality-cosbench
 	pdsh -g s3 docker restart scality-cosbench

 	./submit_sequential_cosbench_workload-s3.sh 

    on UI: 
	10.203.0.11:19088/controller
	view job stats 

    to stop all:  
 	pdsh -g s3 docker stop    scality-cosbench
 	docker stop scality-cosbench

 	./submit_sequential_cosbench_workload-s3.sh 

		 

============================================================================
====PartIII s3cmd              ===============================================
============================================================================

	ping s3.203.pa.sts-asc.net
	s3cmd -c /root/.s3cfg.s3 ls
	s3cmd -c /root/.s3cfg.s3 ls s3://test
	s3cmd -c /root/.s3cfg.s3 ls -l s3://test
	man s3cmd

	# cat  /root/.s3cfg.s3

[default]
access_key = AQGCUA6JB5L2N2T2M3QL
host_base = s3.203.pa.sts-asc.net:8000
host_bucket = %(bucket)s.s3.203.pa.sts-asc.net:8000
secret_key = hvMOo7BSa2CDkfRRvHDLMMby+hp2Jn/v8V7B1Rg7
signature_v2 = True
use_https = False

	to check logs no sut02: 
		cd /tmp/scality-s3/logs
		tail -f s3-0.log



============================================================================
====PartIV  s3load 
============================================================================
1. S3 operations: 
	cd s3 
	./remove-b ruiping-1 
		. remove bucket recursive 
	./com
		. sampling bucket number 
2. s3load 
	load_seq : 1--100 [20] 
	bucket   : 1-1000 [5] 
	rps      : 1--20 [20] 

	changes: 
		A. number of bucket: 1-1000 [10]
		B. python path: /usr/bin/python 
		C. bucket name "_" ==> "-" 
		D. file_set ==> 1 x 512K 
	new: 
		news3load.sh 
			parameters: 
				i 1 	3 
				p 10 	3 
				g 10    3 raw... 100000 
			changes: 
				i ==> nohup 
				take seq 
				"_" ==> "-" 

	   number of load_seq is fixed to 20  (10 bucket/seq)   

		blatchi 
		batchp 20  dir-seq
		batchg 20  dir-seq
			example: dir-seq : 0822-5

	few results: 
		 ./batchp 20 0822-1
			. 400 put/ sec 
			results: 
				 512K       1200         0     68     42    165     90     94     97    117

		 ./batchg 20 0822-1
			. 400 get/ sec 
			results: 
				 524K       1200         0     94     68    114    101    104    107    110


		 ./batchp 25 0823-1
			. 500 put/ sec 
			results: 
				error , connects issue ? 
				
		40 seq:  
		 ./batchp 20 0824-1
			. 800 put /sec 
			 	9000  512K       1200         0    102     58    623    116    126    139    225
			
		 ./batchg 20 0824-1
			. 800 get /sec 
			        6000  524K       1199         0     87     29    112     96     99    102    105

		 ./batchg 1  0824-1
			. 40  get /sec 
				 720  524K         60         0     10      9     20     11     12     13     14
			
		 ./batchp 20 0825-1
			. 800 put /sec 
				 2280  512K       1200         0    311    100    838    348    368    386    423

		 ./batchp 1  0825-2
			. 40  put /sec 
			 	392700  512K         60         0     30     16     55     47     49     50     50
		 ./batchg 4  0825-2
			. 160  put /sec 

	new s3 
		./batchi 
		./batchp 5  0831-1
			. 200 put/sec 

   Seq  Size         Ok       Err    Avg    Min    Max    80%    90%    95%    99%
       60  512K        300         0     29     17    113     30     49     60     70



